---
title: Beelink GTR9 ProでChat-GPT-OSS 120Bを動かしてみた結果──ローカル実行の壁と、それでも惚れた理由
date: "2025-07-05"
dateModified: "2025-07-17"
description: "ChatGPT, Gemini, Claude, GitHub Copilot, JetBrains Junieを、料金、無料枠、IDE連携の観点から比較。"
featuredImagePath: "featured/ai-cording.webp"
nodeType: blog
category: 技術
tags: ["AI", "IDE", "ChatGPT", "Gemini", "Claude", "GitHub Copilot", "JetBrains"]
---

## 1. はじめに：ローカルで120Bを動かしたい！

AI最盛期の今「クラウドではなくローカルでLLMを動かす」無料でAI使い放題を夢見て購入をしました。
Chat-GPT-OSS 120B がオープンウェイトモデルとしては中々パフォーマンスが良さそうなので、をローカルで使いたい理由（コスト／プライバシー／レスポンス）

そのためにBeelink GTR9 Proを選んだ経緯

### → 結論（先出し）

120Bは起動できたが、コンテキスト長を確保するとメモリ不足で破綻。
つまり **動く** けど **実用には至らず** 。
しかし、それでもこの価格帯でこの性能は非常に魅力的で、言い訳がましいが満足感はある。

## 2. Beelink GTR9 Pro 概要レビュー

### ハードウェア仕様

- AMD Ryzen™ AI Max+ 395 Processor
- AMD Radeon™ 8060S GPU (内蔵GPU)
- 128GB RAM LPDDR5x-8000
- 2TB Crucial SSD

特筆すべきは、 **AMD Ryzen™ AI Max+ 395** 搭載と、 **128GBという特大メモリサイズ**
ユニファイドメモリというなのでCPUとGPUで共有する形にはなりますが、それぞれ最大96GBものメモリを使うことができるのが利点です。
GPUは内蔵のものになってしまいスペックは見劣りしますが、低コストで高速大容量のVRAMが使えるという点ではパフォーマンスの利点があります。

:::note info
ユニファイドメモリとは、CPUとGPUが単一のメモリ領域を共有するアーキテクチャのこと。Appleシリコンなどで採用され、CPUとGPUがデータを高速にやり取りできるため、効率とパフォーマンスが向上します。一般的なPCのようにRAMとVRAMが別々にあるのではなく、必要に応じてメモリを動的に融通し合えるのが最大の特徴です。
:::

### 実際の使用感

- **この値段でここまでの潤沢なメモリ性能** という圧倒的コスパの良さ
- 本体サイズと筐体品質

ファン音はとても静かです。基本的に動作音は聞こえないです。

## 3. 商品購入から届くまで

- 2025年8月31日: 注文。$1,985
  予約受注扱いで、35日以内に発送という形。
- 2025年9月24日:
 

## 4. 実験結果：120Bモデルは少し動くけれどすぐにクラッシュ

起動時のメモリ使用量（RAM / VRAM / Swapの実測）

コンテキスト長を伸ばしたときの挙動

すぐに枯渇する

LM Studioがハング／落ちる

ログから見るメモリ不足の実態

試した対策

量子化（4bit, 8bit）

スワップ拡張・仮想メモリ設定

分割ロード・オフロード設定

結果としての限界点：
→ 「動作はするが、120Bでまともにコンテキストを維持できない」

6. コーディング用途としての試用（Codex連携）

ローカルAIでのコード補完体験

小規模モデルでのスムーズさ

120B起動時の挙動と安定性比較

「軽いモデルを複数使う」現実解の提案

## 7. 総評：ローカルAI開発の夢破れても、大容量メモリは優秀
　
Chat-GPT-OSS 120Bを使ったローカルAI開発には届かなかったが、
この価格帯でここまで静か・速い・安定して動作するのは驚異的。
開発用途には十分戦えるマシン
とはいいつつも、CPUのAI最適化機能を使えてはないのでもったいなさはある。
GPU性能も活用できてないが、ゲーミング用途に今後使える可能性はあるか。現状未検証。

今後は「ローカルでどこまで現実的に使えるか」を継続検証していく