---
title: Beelink GTR9 ProでChat-GPT-OSS 120Bを動かしてみた結果──ローカル実行の壁と、それでも惚れた理由
date: "2025-07-05"
dateModified: "2025-07-17"
description: "ChatGPT, Gemini, Claude, GitHub Copilot, JetBrains Junieを、料金、無料枠、IDE連携の観点から比較。"
featuredImagePath: "featured/ai-cording.webp"
nodeType: blog
category: 技術
tags: ["AI", "IDE", "ChatGPT", "Gemini", "Claude", "GitHub Copilot", "JetBrains"]
---

## 1. はじめに：ローカルで120Bを動かしたい！

AI最盛期の今「クラウドではなくローカルでLLMを動かす」無料でAI使い放題を夢見て購入をしました。
Chat-GPT-OSS 120B がオープンウェイトモデルとしては中々パフォーマンスが良さそうなので、をローカルで使いたい理由（コスト／プライバシー／レスポンス）

そのためにBeelink GTR9 Proを選んだ経緯

### → 結論（先出し）

120Bは起動できたが、コンテキスト長を確保するとメモリ不足で破綻。
つまり **動く** けど **実用には至らず** 。
しかし、それでもこの価格帯でこの性能は非常に魅力的で、言い訳がましいが満足感はある。

2. Beelink GTR9 Pro 概要レビュー

ハードウェア仕様まとめ

AMD Ryzen™ AI Max+ 395 Processor

64GB RAM / 2TB NVMe SSD（構成例）

内蔵GPU/NPU構成とAIアクセラレーションの期待

実際の使用感

本体サイズと筐体品質

ファン音・温度・電力効率

**「この値段でここまでの性能」**というコスパの驚き

## 3. 商品購入から届くまで


3. セットアップと環境構築

OS・ドライバ構成（Windows 11 / Linux系の選定理由）

LM Studioの導入

Chat-GPT-OSS 120Bモデルの入手とセットアップ

Codex連携（ローカル推論を開発支援に活かす構想）

4. 実験結果：120Bモデルは“動く”けれど“持たない”

起動時のメモリ使用量（RAM / VRAM / Swapの実測）

コンテキスト長を伸ばしたときの挙動

すぐに枯渇する

LM Studioがハング／落ちる

ログから見るメモリ不足の実態

試した対策

量子化（4bit, 8bit）

スワップ拡張・仮想メモリ設定

分割ロード・オフロード設定

結果としての限界点：
→ 「動作はするが、120Bでまともにコンテキストを維持できない」

5. 比較と考察：現実的なローカルAI構成とは

70Bや13Bモデルとの比較（安定性・実用性）

メモリ帯域とストレージI/Oがボトルネックに

GTR9 Proクラスでの「現実的な最大モデルサイズ」考察

ローカル推論の理想と現実のギャップ

6. コーディング用途としての試用（Codex連携）

ローカルAIでのコード補完体験

小規模モデルでのスムーズさ

120B起動時の挙動と安定性比較

「軽いモデルを複数使う」現実解の提案

7. 総評：GTR9 Proは“敗北”してもなお優秀

Chat-GPT-OSS 120Bには届かなかったが、

この価格帯でここまで静か・速い・安定して動作するのは驚異的

小中規模LLM・開発用途には十分戦えるマシン

「失敗体験が教えてくれた、ローカルAI環境の現実」

8. 今後の展望

次の一手（外部GPU / クラスタ構成 / NPU活用）

LM StudioやOSSモデルの進化への期待

「ローカルAIで自己完結する時代」への布石

9. まとめ

Beelink GTR9 Proは「120Bの壁」にぶつかったが、挑戦の価値はあった

ローカルLLM構築の難しさと、コスパPCの可能性

今後は「ローカルでどこまで現実的に使えるか」を継続検証していく